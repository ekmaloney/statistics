---
title: "Chapter 4 Exercises"
author: "Emily Maloney"
date: "January 28, 2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Chapter 4

```{r libraries, message=FALSE}
library(tidyverse)
library(brms)
library(tidybayes)
```

### Easy Problems

####4E1
In this model, the likelihood is defined by $y~i$ ~ Normal($\mu$, $\sigma$).

####4E2
There are 2 parameters in the posterior distribution of this model.

####4E3
omit

####4E4
The line describing the linear model is $\mu~i$ = $\alpha$ + $\beta~x~i$.

####4E5
There are 3 parameters in the posterior distribution of this model. 

###Medium Problems

####4M1
```{r}
#sampling from both priors to get simulation of observed heights
n <- 1e4
set.seed(432)
tibble(sample_mu = rnorm(n, mean = 0, sd = 10),
       sample_sigma = runif(n, min = 0, max = 10)) %>% 
       mutate(x = rnorm(n, mean = sample_mu, sd = sample_sigma)) %>% 
       
       ggplot(aes(x = x)) + 
       geom_density(fill = "black", size = 0) +
       labs(subtitle = expression(paste("Prior predictive distribution for ", italic(h[i]))),
            x = NULL)
```
Simulating observed heights from the prior information results in a distribution centered at 0, which is expected, given that the prior specification for $\mu$ is a normal distribution with a mean of 0. 

####4M2
The model translated into a map formula is:   
flist <- alist (  
        y ~ dnorm(mu, sigma),  
        mu ~ dnorm(0, 10),  
        sigma ~ dunif(0, 10)  
)  

####4M3
The map model formula translated into a mathematical model definition is:  
      $y~i$ ~ Normal($\mu$, $\sigma$)  
      $\mu~i$ = $\alpha$ + $\beta~x~i$  
      $\alpha$ ~ Normal(0, 50)  
      $\beta$ ~ Uniform(0, 10)  
      $\sigma$ ~ Uniform(0, 50)  
      
####4M4
The mathematical model definitions for predicting height using year as a predictor I would use is:  
      $y~i$ ~ Normal($\mu$, $\sigma$)  
      $\mu~i$ = $\alpha$ + $\beta~x~i$  
      $\alpha$ ~ Normal(107, 10)   
      $\beta$ ~ Normal(7, 2)  
      $\sigma$ ~ Uniform(0, 25)  

For the specification of alpha, I was assuming that the students would be kindergarteners during the first year of observation, so I guessed that the mean would be around 3.5 feet, which is ~107 centimeters, and a standard deviation of 10 centimeters, because that allows for a fair amount of variation in heights of kindergarteners. Considering that children grow fairly quickly, I then decided that the prior for beta should have a mean of 3 inches, which is around 7 centimeters, and a standard deviation of 1, because most kids grow at fairly similar rates at that yonug age. Finally, to specify the prior for sigma, I did not have strong assumptions or knowledge about what sigma overall would be, so I specified a uniform distribution going from 0 to 25. 

####4M5
In this situation, I would not change my prior, because this information is the actual data and will instead be used to fit the model with the priors as they are. 

####4M6
In this case, this new information is additional prior information and not data from the sample, so I would change the prior specification for $\sigma$ (standard deviation) to $\sigma$ ~ Uniform(0, 8).
  
    
      
###Hard Problems

####4H1
```{r, message=FALSE}
library(rethinking)
library(tidyverse)
library(knitr)
data(Howell1) # load in data

d <- Howell1
#d2 <- d %>% filter(age >= 18) # filter to only adults

#fit model
mhw.1 <- rethinking::map(alist(
             height ~ dnorm(mu, sigma),
             mu <- a + b*weight,
             a ~ dnorm(156, 100),
             b ~ dnorm(0, 10),
             sigma ~ dunif(0, 50)
            ), 
         data = d)
precis(mhw.1)

N <- 1e4 # sample size

# Get predictive means and data
preds <- 
  as_tibble(MASS::mvrnorm(mu = mhw.1@coef,
                          Sigma = mhw.1@vcov , n = N )) %>%      # rather than extract.samples
  mutate(weight = sample(c(46.95, 43.72, 64.78, 32.59, 54.63), N, replace = T),
         predmean = a + b * weight ,                            # line uncertainty
         predverb = rnorm(N, a + b*weight, sigma )) %>%         # data uncertainty
  group_by(weight) %>% 
  mutate(lb_mu = rethinking::HPDI(predmean, prob = .89)[1],
         ub_mu = rethinking::HPDI(predmean, prob = .89)[2],
         lb_ht = rethinking::HPDI(predverb, prob = .89)[1],
         ub_ht = rethinking::HPDI(predverb, prob = .89)[2]) %>% 
  slice(1) %>%
  mutate(yhat = mhw.1@coef["a"] + mhw.1@coef["b"] * weight) %>%       # yhat for reg line
  select(weight, yhat,lb_ht, ub_ht)

kable(preds, type = "pandoc", caption = "!Kung Predicted Heights")
```

Using the model's specification of $\beta$ = 1.76, $\alpha$ = 75.44, and $\sigma$ = 9.35, the expected heights and 89% intervals for these individuals were produced by simulating from the posterior distribution of the model and are shown in the table above. 

####4H2
  a)
```{r}
#filter data to only children
d3 <- d %>% filter(age < 18)

#fit model
mhw.2 <- rethinking::map(alist(
             height ~ dnorm(mu, sigma),
             mu <- a + b*weight,
             a ~ dnorm(156, 100),
             b ~ dnorm(0, 10),
             sigma ~ dunif(0, 50)
            ), 
         data = d3)

#summary call for what's in the model
precis(mhw.2)

```

For every 10 units increase in weight, the model predicts that a child will get 27.2 cm taller. 

b)
```{r}
N <- 1e6 # sample size

# Get predictive means and data
preds <- 
  as.tibble(MASS::mvrnorm(mu = mhw.2@coef,
                          Sigma = mhw.2@vcov , n = N )) %>%      # rather than extract.samples
  mutate(weight = sample(seq(from = 4.25, to = 44.75, by = 0.1), N, replace = T),
         predmean = a + b * weight ,                            # line uncertainty
         predverb = rnorm(N, a + b*weight, sigma )) %>%         # data uncertainty
  group_by(weight) %>% 
  mutate(lb_mu = rethinking::HPDI(predmean, prob = .89)[1],
         ub_mu = rethinking::HPDI(predmean, prob = .89)[2],
         lb_ht = rethinking::HPDI(predverb, prob = .89)[1],
         ub_ht = rethinking::HPDI(predverb, prob = .89)[2]) %>% 
  slice(1) %>%
  mutate(yhat = mhw.2@coef["a"] + mhw.2@coef["b"] * weight) %>%       # yhat for reg line
  select(weight, yhat, lb_mu, ub_mu, lb_ht, ub_ht)

#plot
ggplot(d3, aes(x = weight)) +
  geom_jitter(aes(y = height), alpha = .3) +
  geom_line(data = preds, aes(y = yhat)) +
  geom_ribbon(data = preds, aes(ymin = lb_mu, ymax = ub_mu), alpha = .3) +
  geom_ribbon(data = preds, aes(ymin = lb_ht, ymax = ub_ht), alpha = .2) +
  labs(x = "Weight",
       y = "Height",
       title = "Predicted Height of !Kung Children")

```

c)
The most concerning aspect of model fit is that a good bit of the data at the highest and lowest weights are not included in the 89% HPDI, and most of the data at middle weights seem to be falling above the MAP regression line although still in the 89% HPDI. Overall, the shape looks more curvilinear than linear, so I hypothesize that adding a squared weight term may result in a better fitting model. 

####4H3
```{r}
#add variable of log weight
d <- d%>% mutate(logweight = log(weight))

#fit model
mhw.3 <- rethinking::map(alist(
             height ~ dnorm(mu, sigma),
             mu <- a + b*logweight,
             a ~ dnorm(178, 100),
             b ~ dnorm(0, 10),
             sigma ~ dunif(0, 50)
            ), 
         data = d)

#summary call for what's in the model
precis(mhw.3)

47.01*log(101/100)
```

For every 1% increase in weight, we expect a 0.468 centimeter increase in height. 

b)
```{r}
N <- 1e6 # sample size

# Get predictive means and data
preds <- 
  as.tibble(MASS::mvrnorm(mu = mhw.3@coef,
                          Sigma = mhw.3@vcov , n = N )) %>%      # rather than extract.samples
  mutate(logweight = sample(seq(from = 1.4, to = 4.15, by = 0.01), N, replace = T),
         predmean = a + b * logweight,                            # line uncertainty
         predverb = rnorm(N, a + b*logweight, sigma )) %>%         # data uncertainty
  group_by(logweight) %>% 
  mutate(lb_mu = rethinking::HPDI(predmean, prob = .89)[1],
         ub_mu = rethinking::HPDI(predmean, prob = .89)[2],
         lb_ht = rethinking::HPDI(predverb, prob = .89)[1],
         ub_ht = rethinking::HPDI(predverb, prob = .89)[2]) %>% 
  slice(1) %>%
  mutate(yhat = mhw.3@coef["a"] + mhw.3@coef["b"] * logweight) %>%       # yhat for reg line
  select(logweight, yhat, lb_mu, ub_mu, lb_ht, ub_ht)

#plot
ggplot(data = d, aes(x = logweight)) +
  geom_jitter(aes(y = height), alpha = .3) +
  geom_line(data = preds, aes(y = yhat)) +
  geom_ribbon(data = preds, aes(ymin = lb_mu, ymax = ub_mu), alpha = .3) +
  geom_ribbon(data = preds, aes(ymin = lb_ht, ymax = ub_ht), alpha = .2) +
  labs(x = "Log Weight",
       y = "Height",
       title = "Predicted Height of !Kung, by Log Weight")
```

This plot of the model's MAP regression line and 89% HPDI interval with the actual data superimposed on top looks like a better fit than the previous model, considering that now the vast majority of the data points fall inside the 89% HPDI and appear to follow the regression line to a greater extent. 